{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21acf48-994c-47ee-af93-8025f9b78719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import rdflib\n",
    "from rdflib import Graph, URIRef, Literal, Namespace, BNode, Dataset\n",
    "from rdflib.namespace import SKOS, DCTERMS, DCMITYPE, RDF, RDFS, XSD, PROV, SDO, TIME, split_uri\n",
    "\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6e0afa-aed6-4b58-a057-55f34e1e317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening config file, the config structure is:\n",
    "# {\"openai_api_key\":\"......\"}\n",
    "\n",
    "config = open('config', 'r')\n",
    "config = json.load(config)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config['openai_api_key']\n",
    "os.environ['GEMINI_API_KEY'] = config['gemini_api_key']\n",
    "os.environ['XAI_API_KEY'] = config['xai_api_key']\n",
    "os.environ['NVIDIA_API_KEY'] = config['nvidia_api_key']\n",
    "os.environ['DEEPSEEK_API_KEY'] = config['deepseek_api_key']\n",
    "os.environ['ANTHROPIC_API_KEY'] = config['claude_api_key']\n",
    "os.environ['DASHSCOPE_API_KEY'] = config['dashscope_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3308a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(data):\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(data=data, format=\"turtle\")\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcd845b-c183-4d6b-9bb5-bdbc96efd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rdf(rdf):\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(data=rdf, format=\"turtle\")\n",
    "\n",
    "    for s, p, o in g:\n",
    "        print(s, p, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9838b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespaces\n",
    "she = Namespace(\"https://soilwise-he.github.io/soil-health#\")\n",
    "agrovoc = Namespace(\"http://aims.fao.org/aos/agrovoc/\")\n",
    "agrontology = Namespace(\"http://aims.fao.org/aos/agrontology#\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "glosis_lh = Namespace(\"http://w3id.org/glosis/model/layerhorizon/\")\n",
    "glosis_sp = Namespace(\"http://w3id.org/glosis/model/siteplot/\")\n",
    "qudt = Namespace(\"http://qudt.org/schema/qudt/\")\n",
    "unit = Namespace(\"http://qudt.org/vocab/unit/\")\n",
    "iso11074 = Namespace(\"https://data.geoscience.earth/ncl/ISO11074v2025/\")\n",
    "obo = Namespace(\"http://purl.obolibrary.org/obo/\")\n",
    "wdt = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
    "biolink = Namespace(\"https://w3id.org/biolink/vocab/\")\n",
    "afox = Namespace(\"http://purl.allotrope.org/ontologies/property#\")\n",
    "afor = Namespace(\"http://purl.allotrope.org/ontologies/result#\")\n",
    "sorelsc = Namespace(\"http://sweetontology.net/relaSci/\")\n",
    "sorelpr = Namespace(\"http://sweetontology.net/relaProvenance/\")\n",
    "sohuj = Namespace(\"http://sweetontology.net/humanJurisdiction/\")\n",
    "sorelph = Namespace(\"http://sweetontology.net/relaPhysical/\")\n",
    "sorelm = Namespace(\"http://sweetontology.net/relaMath/\")\n",
    "sorepsg = Namespace(\"http://sweetontology.net/reprSpaceGeometry/\")\n",
    "bao = Namespace(\"http://www.bioassayontology.org/bao#\")\n",
    "repr = Namespace(\"https://w3id.org/reproduceme#\")\n",
    "sorelch = Namespace(\"http://sweetontology.net/relaChemical/\")\n",
    "sorelsp = Namespace(\"http://sweetontology.net/relaSpace/\")\n",
    "om = Namespace(\"http://www.ontology-of-units-of-measure.org/resource/om-2/\")\n",
    "afop = Namespace(\"http://purl.allotrope.org/ontologies/process#\")\n",
    "gemet = Namespace(\"http://www.eionet.europa.eu/gemet/concept/\")\n",
    "inrae = Namespace(\"http://opendata.inrae.fr/thesaurusINRAE/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ada89",
   "metadata": {},
   "source": [
    "### Vocabs or not vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd04df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 10990 triples from soil_health_KG.ttl\n",
      "Found 494 unique concepts with exactMatch or closeMatch properties\n",
      "Successfully saved concept URIs to skos_concepts_with_matches.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_skos_concepts_with_matches(ttl_file_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Extract SKOS concepts that have exactMatch or closeMatch properties\n",
    "    and save their URIs to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        ttl_file_path (str): Path to the TTL file containing the RDF knowledge graph\n",
    "        output_csv_path (str): Path where the CSV file will be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a graph and load the TTL file\n",
    "    g = Graph()\n",
    "    try:\n",
    "        g.parse(ttl_file_path, format='turtle')\n",
    "        print(f\"Successfully loaded {len(g)} triples from {ttl_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading TTL file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Define SKOS namespace\n",
    "    SKOS = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "    \n",
    "    # Set to store unique concept URIs\n",
    "    concepts_with_matches = set()\n",
    "    \n",
    "    # Query for concepts with exactMatch\n",
    "    exact_match_concepts = g.subjects(SKOS.exactMatch, None)\n",
    "    for concept in exact_match_concepts:\n",
    "        if isinstance(concept, URIRef):\n",
    "            concepts_with_matches.add(str(concept))\n",
    "    \n",
    "    # Query for concepts with closeMatch\n",
    "    close_match_concepts = g.subjects(SKOS.closeMatch, None)\n",
    "    for concept in close_match_concepts:\n",
    "        if isinstance(concept, URIRef):\n",
    "            concepts_with_matches.add(str(concept))\n",
    "    \n",
    "    # Convert to sorted list for consistent output\n",
    "    concepts_list = sorted(list(concepts_with_matches))\n",
    "    \n",
    "    print(f\"Found {len(concepts_list)} unique concepts with exactMatch or closeMatch properties\")\n",
    "    \n",
    "    # Save to CSV file\n",
    "    try:\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            # Write header\n",
    "            writer.writerow(['concept_uri'])\n",
    "            # Write concept URIs\n",
    "            for concept_uri in concepts_list:\n",
    "                writer.writerow([concept_uri])\n",
    "        \n",
    "        print(f\"Successfully saved concept URIs to {output_csv_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV file: {e}\")\n",
    "        return\n",
    "    \n",
    "    return concepts_list\n",
    "\n",
    "def extract_with_match_details(ttl_file_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Alternative version that also extracts the match details (what each concept matches to)\n",
    "    and the type of match (exact or close).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a graph and load the TTL file\n",
    "    g = Graph()\n",
    "    try:\n",
    "        g.parse(ttl_file_path, format='turtle')\n",
    "        print(f\"Successfully loaded {len(g)} triples from {ttl_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading TTL file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Define SKOS namespace\n",
    "    SKOS = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "    \n",
    "    # List to store detailed match information\n",
    "    match_details = []\n",
    "    \n",
    "    # Query for exactMatch relationships\n",
    "    for subject, predicate, obj in g.triples((None, SKOS.exactMatch, None)):\n",
    "        if isinstance(subject, URIRef):\n",
    "            match_details.append({\n",
    "                'concept_uri': str(subject),\n",
    "                'match_type': 'exactMatch',\n",
    "                'matched_uri': str(obj)\n",
    "            })\n",
    "    \n",
    "    # Query for closeMatch relationships\n",
    "    for subject, predicate, obj in g.triples((None, SKOS.closeMatch, None)):\n",
    "        if isinstance(subject, URIRef):\n",
    "            match_details.append({\n",
    "                'concept_uri': str(subject),\n",
    "                'match_type': 'closeMatch',\n",
    "                'matched_uri': str(obj)\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(match_details)} total match relationships\")\n",
    "    \n",
    "    # Save detailed information to CSV\n",
    "    try:\n",
    "        df = pd.DataFrame(match_details)\n",
    "        df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"Successfully saved detailed match information to {output_csv_path}\")\n",
    "        \n",
    "        # Also print summary statistics\n",
    "        unique_concepts = df['concept_uri'].nunique()\n",
    "        exact_matches = len(df[df['match_type'] == 'exactMatch'])\n",
    "        close_matches = len(df[df['match_type'] == 'closeMatch'])\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"- Unique concepts with matches: {unique_concepts}\")\n",
    "        print(f\"- Total exactMatch relationships: {exact_matches}\")\n",
    "        print(f\"- Total closeMatch relationships: {close_matches}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV file: {e}\")\n",
    "        return\n",
    "    \n",
    "    return match_details\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Basic version - just concept URIs\n",
    "    ttl_file = \"soil_health_KG.ttl\"  # Replace with your TTL file path\n",
    "    output_csv = \"skos_concepts_with_matches.csv\"\n",
    "    \n",
    "    concepts = extract_skos_concepts_with_matches(ttl_file, output_csv)\n",
    "    \n",
    "    # Detailed version - with match information\n",
    "    # Uncomment the lines below if you want detailed match information\n",
    "    # detailed_output_csv = \"skos_concepts_detailed_matches.csv\"\n",
    "    # match_details = extract_with_match_details(ttl_file, detailed_output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8019c135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CSV with 684 rows\n",
      "Number of unique URIs in keywords column: 683\n",
      "Number of unique URIs in thesauri column: 494\n",
      "\n",
      "Set operation results:\n",
      "Union (all unique URIs): 788\n",
      "Intersection (URIs in both columns): 389\n",
      "Keywords only: 294\n",
      "Thesauri only: 105\n",
      "Saved 788 URIs to ./uri_union.csv\n",
      "Saved 389 URIs to ./uri_intersection.csv\n",
      "Saved 294 URIs to ./uri_keywords_only.csv\n",
      "Saved 105 URIs to ./uri_thesauri_only.csv\n",
      "\n",
      "All files saved successfully to directory: ./\n"
     ]
    }
   ],
   "source": [
    "def analyze_uri_sets(input_csv_path, output_dir=\"./\"):\n",
    "    \"\"\"\n",
    "    Analyze URIs from two columns and create separate CSV files for each set operation result.\n",
    "    \n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file\n",
    "        output_dir (str): Directory to save output files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        print(f\"Successfully loaded CSV with {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'keywords' not in df.columns or 'thesauri' not in df.columns:\n",
    "        print(\"Error: Required columns 'keywords' and 'thesauri' not found in CSV\")\n",
    "        return\n",
    "    \n",
    "    # Remove NaN values and convert to sets\n",
    "    keywords_set = set(df['keywords'].dropna().astype(str))\n",
    "    thesauri_set = set(df['thesauri'].dropna().astype(str))\n",
    "    \n",
    "    print(f\"Number of unique URIs in keywords column: {len(keywords_set)}\")\n",
    "    print(f\"Number of unique URIs in thesauri column: {len(thesauri_set)}\")\n",
    "    \n",
    "    # Perform set operations\n",
    "    union_set = keywords_set.union(thesauri_set)\n",
    "    intersection_set = keywords_set.intersection(thesauri_set)\n",
    "    keywords_only = keywords_set - thesauri_set\n",
    "    thesauri_only = thesauri_set - keywords_set\n",
    "    \n",
    "    print(f\"\\nSet operation results:\")\n",
    "    print(f\"Union (all unique URIs): {len(union_set)}\")\n",
    "    print(f\"Intersection (URIs in both columns): {len(intersection_set)}\")\n",
    "    print(f\"Keywords only: {len(keywords_only)}\")\n",
    "    print(f\"Thesauri only: {len(thesauri_only)}\")\n",
    "    \n",
    "    # Save each set to a separate CSV file\n",
    "    sets_data = {\n",
    "        'union': union_set,\n",
    "        'intersection': intersection_set,\n",
    "        'keywords_only': keywords_only,\n",
    "        'thesauri_only': thesauri_only\n",
    "    }\n",
    "    \n",
    "    for set_name, uri_set in sets_data.items():\n",
    "        filename = f\"{output_dir}uri_{set_name}.csv\"\n",
    "        try:\n",
    "            df_temp = pd.DataFrame({'URI': sorted(uri_set)})\n",
    "            df_temp.to_csv(filename, index=False)\n",
    "            print(f\"Saved {len(uri_set)} URIs to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {filename}: {e}\")\n",
    "    \n",
    "    print(f\"\\nAll files saved successfully to directory: {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual file paths\n",
    "    input_file = \"matched_concepts.csv\"\n",
    "    output_directory = \"./\"  # Current directory, change as needed\n",
    "    \n",
    "    analyze_uri_sets(input_file, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d2b3d",
   "metadata": {},
   "source": [
    "#### LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68f8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_ee = \"\"\"You are a highly specialized entity extraction agent. Your primary function is to analyze a user's natural language query and extract all mentions of entities (people, organizations, dates, products, technical concepts, geographical locations, etc.). The ultimate goal is to use these entities to sample a subgraph from a domain knowledge graph.\n",
    "\n",
    "Your task is to identify entities and provide a standardized canonical term for each entity that would exist in a formal knowledge graph.\n",
    "\n",
    "**# Extraction & Standardization Rules**\n",
    "\n",
    "1. **Identify Entities:** Carefully read the user's query and identify all possible entities.\n",
    "2. **Preserve Original Text:** For each entity found, you must capture the exact text as it appeared in the query.\n",
    "3. **Standardize Term:** For each entity, you must also provide a standardized, canonical term. This term should be the most common or formal name for the entity, as one might find in a knowledge base or encyclopedia.\n",
    "\n",
    "**# Output Format**\n",
    "\n",
    "Your output MUST be a valid JSON array of objects.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f46902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cq = f\"\"\"Now please extract all entities from the following query:\n",
    "'{qa_pairs[23]['competency_question']}'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc1e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"user_query\":\"What is SOM and what is it made from, and how to convert measured SOC to SOM?\",\"extracted_entities\":[{\"entity\":\"SOM\",\"standardized_term\":\"Soil Organic Matter\"},{\"entity\":\"SOC\",\"standardized_term\":\"Soil Organic Carbon\"}]}\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt_ee},\n",
    "        {\"role\": \"user\", \"content\": prompt_cq}\n",
    "    ],\n",
    "    response_format={ \n",
    "        \"type\": \"json_schema\", \n",
    "        \"json_schema\": {\n",
    "            \"name\": \"entity_extraction\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"user_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The natural language query provided by the user for which entities need to be extracted.\",\n",
    "                        \"minLength\": 1\n",
    "                        },\n",
    "                    \"extracted_entities\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"A list of entities extracted from the user query.\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"entity\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"The original entity extracted from the query.\"\n",
    "                                    },\n",
    "                                \"standardized_term\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"A standardized term corresponding to the original entity, possibly inferred.\"\n",
    "                                    }\n",
    "                                  },\n",
    "                          \"required\": [\n",
    "                              \"entity\",\n",
    "                              \"standardized_term\"\n",
    "                              ],\n",
    "                          \"additionalProperties\": False\n",
    "                        }\n",
    "                      }\n",
    "                    },\n",
    "                \"required\": [\n",
    "                    \"user_query\",\n",
    "                    \"extracted_entities\"\n",
    "                    ],\n",
    "                \"additionalProperties\": False\n",
    "              },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef2d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'SOM', 'standardized_term': 'Soil Organic Matter'}, {'entity': 'SOC', 'standardized_term': 'Soil Organic Carbon'}]\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(completion.choices[0].message.content)[\"extracted_entities\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
